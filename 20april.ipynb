{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83kAa06IAjo6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the KNN algorithm?\n",
        "\n",
        "Ans:The K-Nearest Neighbors (KNN) algorithm is a non-parametric, instance-based learning algorithm used for both classification and regression tasks. In classification, KNN assigns the class label that is most common among the K nearest data points to a given test point. In regression, it predicts the target variable as the average of the values of the K nearest neighbors. KNN works by measuring the similarity (or distance) between the test point and all other points in the training dataset, using a distance metric like Euclidean distance, and then selecting the K closest neighbors to make a prediction."
      ],
      "metadata": {
        "id": "qAdFtzUHAoMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. How do you choose the value of K in KNN?\n",
        "\n",
        "Ans:Choosing the value of K (the number of nearest neighbors) is crucial for the performance of the KNN algorithm. A small value of K (such as 1) can lead to high variance and overfitting, as the model might be too sensitive to noise in the training data. A larger value of K smoothens the decision boundary but may result in high bias and underfitting, especially if K is too large. The optimal value of K is typically chosen through methods like cross-validation or using techniques such as grid search. A common practice is to try different values of K and select the one that minimizes the error or maximizes performance metrics (like accuracy for classification)."
      ],
      "metadata": {
        "id": "iqRjrncTAslS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is the difference between KNN classifier and KNN regressor?\n",
        "\n",
        "Ans:The KNN classifier is used for classification tasks, where the goal is to assign a class label to a test point based on the majority vote of its K nearest neighbors. For example, it can be used to classify an email as spam or not based on the labels of nearby emails. In contrast, the KNN regressor is used for regression tasks, where the goal is to predict a continuous value. Instead of voting for the class, the KNN regressor computes the average of the values of the K nearest neighbors to make a prediction. The main difference lies in the type of target variable: categorical for classification and continuous for regression."
      ],
      "metadata": {
        "id": "Gf5BI8XTAvaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. How do you measure the performance of KNN?\n",
        "\n",
        "Ans:\n",
        "The performance of a KNN model can be measured using various metrics, depending on whether the task is classification or regression. For classification, common evaluation metrics include:\n",
        "\n",
        "Accuracy: The proportion of correctly classified instances.\n",
        "Precision: The ratio of true positive instances to the total predicted positives.\n",
        "Recall: The ratio of true positive instances to the total actual positives.\n",
        "F1-score: The harmonic mean of precision and recall.\n",
        "For regression, performance is typically evaluated using:\n",
        "\n",
        "Mean Squared Error (MSE): The average of the squared differences between predicted and actual values.\n",
        "Mean Absolute Error (MAE): The average of the absolute differences between predicted and actual values.\n",
        "R-squared (R¬≤): A measure of how well the model‚Äôs predictions match the actual values, representing the proportion of variance explained by the model."
      ],
      "metadata": {
        "id": "j19aI81xA0PS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is the curse of dimensionality in KNN?\n",
        "\n",
        "Ans:The curse of dimensionality refers to the phenomenon where the performance of the KNN algorithm deteriorates as the number of features (dimensions) in the dataset increases. In high-dimensional spaces, data points become sparse, and the notion of \"closeness\" or distance becomes less meaningful, as all points tend to become equidistant from each other. This causes KNN to lose its effectiveness because finding the true nearest neighbors becomes more difficult and computationally expensive. To mitigate this, dimensionality reduction techniques like Principal Component Analysis (PCA) or feature selection can be applied to reduce the number of features."
      ],
      "metadata": {
        "id": "lcryw30VA3_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Q6. How do you handle missing values in KNN?\n",
        "\n",
        "Ans:Handling missing values in KNN is important to ensure that the algorithm can make accurate predictions. There are several strategies:\n",
        "\n",
        "Imputation: Missing values can be imputed with a reasonable estimate, such as the mean, median, or mode of the feature. For categorical data, the most frequent value (mode) is often used.\n",
        "Using KNN for Imputation: Another approach is to use KNN itself to impute missing values by finding the nearest neighbors and averaging their values for numerical data or selecting the most frequent value for categorical data.\n",
        "Removing Data Points: If the missing data is sparse and does not significantly affect the overall dataset, rows or columns with missing values can be dropped, though this may not be ideal for large datasets."
      ],
      "metadata": {
        "id": "GhApoj6GA9Yx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bU-kLoonBBxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
        "\n",
        "Ans:\n",
        "The KNN classifier is best suited for classification tasks where the goal is to assign labels to instances, such as in spam detection or medical diagnoses. It works well when the classes are well-separated and when the dataset is relatively small. The KNN regressor, on the other hand, is better suited for regression tasks where the goal is to predict a continuous value, such as in stock price prediction or temperature forecasting. In general, both classifiers and regressors perform well with low-dimensional, clean data but can struggle with high-dimensional data due to the curse of dimensionality. The choice between classifier and regressor depends on the nature of the target variable (categorical for classification or continuous for regression)."
      ],
      "metadata": {
        "id": "wfSygD9oBDJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
        "\n",
        "\n",
        "\n",
        "Ans:Strengths of KNN:\n",
        "\n",
        "Simple and easy to understand and implement.\n",
        "Non-parametric, meaning it does not make assumptions about the underlying data distribution.\n",
        "Performs well for small datasets with few features.\n",
        "Weaknesses of KNN:\n",
        "\n",
        "Computationally expensive, as it requires calculating distances to all training points for each prediction.\n",
        "Sensitive to irrelevant or redundant features, which can degrade performance.\n",
        "Struggles with high-dimensional data (curse of dimensionality) and noisy data.\n",
        "These weaknesses can be addressed by:\n",
        "\n",
        "Feature scaling to normalize the range of features.\n",
        "Dimensionality reduction techniques (such as PCA) to reduce the number of features and mitigate the curse of dimensionality.\n",
        "Optimizing the value of K and selecting appropriate distance metrics to improve accuracy.\n",
        "Data preprocessing to handle missing values, noise, and outliers before training the model."
      ],
      "metadata": {
        "id": "WW4cahvNBGPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
        "\n",
        "Ans:\n",
        "Euclidean distance and Manhattan distance are two commonly used distance metrics in KNN:\n",
        "\n",
        "Euclidean distance is the straight-line distance between two points in a Euclidean space. It is calculated as the square root of the sum of squared differences between corresponding coordinates. This metric is sensitive to outliers, as large differences in a single dimension can significantly affect the overall distance.\n",
        "\n",
        "Formula:\n",
        "\n",
        "ùëë\n",
        "=\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        "‚àí\n",
        "ùë¶\n",
        "ùëñ\n",
        ")\n",
        "2\n",
        "d=\n",
        "i=1\n",
        "‚àë\n",
        "n\n",
        "‚Äã\n",
        " (x\n",
        "i\n",
        "‚Äã\n",
        " ‚àíy\n",
        "i\n",
        "‚Äã\n",
        " )\n",
        "2\n",
        "\n",
        "‚Äã\n",
        "\n",
        "Manhattan distance, also known as L1 distance, measures the distance between two points by summing the absolute differences of their coordinates. It is more robust to outliers compared to Euclidean distance.\n",
        "\n",
        "Formula:\n",
        "\n",
        "ùëë\n",
        "=\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "‚à£\n",
        "ùë•\n",
        "ùëñ\n",
        "‚àí\n",
        "ùë¶\n",
        "ùëñ\n",
        "‚à£\n",
        "d=\n",
        "i=1\n",
        "‚àë\n",
        "n\n",
        "‚Äã\n",
        " ‚à£x\n",
        "i\n",
        "‚Äã\n",
        " ‚àíy\n",
        "i\n",
        "‚Äã\n",
        " ‚à£\n",
        "Euclidean distance is typically used when the data is continuous and does not have large variations, while Manhattan distance is preferred when the data consists of discrete features or when the dataset is noisy."
      ],
      "metadata": {
        "id": "EukytCDyBL7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. What is the role of feature scaling in KNN?\n",
        "\n",
        "\n",
        "Ans:\n",
        "Feature scaling plays a crucial role in KNN because the algorithm relies on calculating distances between data points. If the features have different scales, features with larger ranges will dominate the distance calculation, leading to biased results. For example, if one feature represents age (with values between 0 and 100) and another represents income (ranging from 10,000 to 100,000), the income feature will disproportionately affect the distance calculation.\n",
        "\n",
        "To address this, it is important to scale the features so that they all contribute equally to the distance calculation. Common methods for feature scaling include:\n",
        "\n",
        "Min-Max scaling, which normalizes the features to a range between 0 and 1.\n",
        "Standardization (Z-score normalization), which transforms the data to have a mean of 0 and a standard deviation of 1.\n",
        "By applying feature scaling, KNN can more accurately measure the \"closeness\" between data points, leading to better performance."
      ],
      "metadata": {
        "id": "KIVSHxzeBQpp"
      }
    }
  ]
}
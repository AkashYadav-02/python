{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Gradient Boosting Regression?\n",
        "\n",
        "Ans:Gradient Boosting Regression is an ensemble learning method used for regression tasks, where multiple weak models (typically decision trees) are trained sequentially to improve the accuracy of predictions. The key idea is that each model corrects the errors of its predecessor. It is based on the gradient boosting framework, which minimizes a loss function by iteratively adding models that predict the residual errors from previous iterations. The final prediction is a combination of all the individual models, and it uses gradient descent to optimize the parameters of the models in the ensemble."
      ],
      "metadata": {
        "id": "6fTLriyR04LL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
      ],
      "metadata": {
        "id": "RfaYZWIx07yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate a simple regression dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1)  # 100 samples, 1 feature\n",
        "y = 3 * X.squeeze() + np.random.randn(100) * 0.5  # Linear relation with noise\n",
        "\n",
        "# Simple decision tree regressor (a weak learner)\n",
        "class SimpleTreeRegressor:\n",
        "    def __init__(self, max_depth=1):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Simple regression tree with one split based on the median of X\n",
        "        self.split_value = np.median(X)\n",
        "        self.mean_value = np.mean(y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.full(X.shape[0], self.mean_value)\n",
        "\n",
        "# Gradient Boosting from scratch\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.models = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize the predictions with the mean of y\n",
        "        y_pred = np.full_like(y, np.mean(y), dtype=np.float64)\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Calculate the residuals (errors)\n",
        "            residuals = y - y_pred\n",
        "\n",
        "            # Fit a model to the residuals\n",
        "            model = SimpleTreeRegressor(max_depth=1)\n",
        "            model.fit(X, residuals)\n",
        "            self.models.append(model)\n",
        "\n",
        "            # Update the predictions\n",
        "            y_pred += self.learning_rate * model.predict(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "        for model in self.models:\n",
        "            y_pred += self.learning_rate * model.predict(X)\n",
        "        return y_pred\n",
        "\n",
        "# Train the Gradient Boosting model\n",
        "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
        "gb.fit(X, y)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = gb.predict(X)\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'R-squared: {r2}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcn09gR80-rc",
        "outputId": "58be0caa-ea34-4bfb-d73c-34ab85f670a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 2.86206771070548\n",
            "R-squared: -2.2748196619436927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimize the performance of the model. Use grid search or random search to find the best hyperparameters."
      ],
      "metadata": {
        "id": "dxKm_Llp9vup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Create a simple regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
        "\n",
        "# Set the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [1, 3, 5]\n",
        "}\n",
        "\n",
        "# Instantiate the GradientBoostingRegressor\n",
        "gbr = GradientBoostingRegressor()\n",
        "\n",
        "# Set up the grid search with cross-validation\n",
        "grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best hyperparameters and score\n",
        "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Best CV Score: {grid_search.best_score_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRHx4ZJb9w03",
        "outputId": "9805fb40-2492-4241-9e57-2945a411759f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 150}\n",
            "Best CV Score: -10.392860545614218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is a weak learner in Gradient Boosting?\n",
        "\n",
        "Ans:A weak learner in gradient boosting is a model that performs slightly better than random guessing. In the context of gradient boosting, the weak learner is typically a decision tree with a limited depth (often a shallow tree). The weak learner is iteratively trained to predict the residuals (errors) from previous models, and its output is used to correct the overall modelâ€™s predictions. A weak learner in gradient boosting does not necessarily make accurate predictions on its own but contributes to the final model by focusing on correcting previous errors."
      ],
      "metadata": {
        "id": "jDsrqCDc9393"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
        "\n",
        "Ans:The intuition behind Gradient Boosting is to iteratively improve a model by focusing on its mistakes. Initially, a simple model (often predicting the mean of the target) is used to make predictions. Then, the residuals (errors) between the predicted and actual values are calculated, and a new model is trained to predict these residuals. The predictions of all models are then combined, and the process repeats, each new model correcting the errors made by the previous models. By focusing on the residuals, gradient boosting effectively minimizes the loss function using gradient descent, leading to a highly accurate ensemble model."
      ],
      "metadata": {
        "id": "2GBgoS0d95vH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
        "\n",
        "Ans:\n",
        "The Gradient Boosting algorithm builds an ensemble of weak learners by training them sequentially. In each step, a new weak learner (typically a decision tree) is trained to predict the residual errors of the previous model. The output of the weak learners is combined in a weighted manner, with each learner contributing based on its accuracy. The algorithm gradually improves the ensemble by focusing on harder-to-predict samples, thus reducing the overall error. The final ensemble prediction is made by summing the predictions of all weak learners, typically using a weighted average for regression tasks."
      ],
      "metadata": {
        "id": "GX9rOpct987K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
        "\n",
        "Ans:\n",
        "The steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm are:\n",
        "\n",
        "Initial Model: Start with an initial model, usually predicting the mean of the target variable.\n",
        "Calculate Residuals: Compute the residuals (errors) between the predicted values and actual values.\n",
        "Fit a Weak Learner: Train a weak learner (e.g., decision tree) to predict these residuals.\n",
        "Update the Model: Update the model by adding the prediction of the weak learner, scaled by a learning rate, to the current model.\n",
        "Repeat: Repeat the process for a set number of iterations, each time fitting a new model to the residuals from the previous iteration.\n",
        "Final Prediction: The final prediction is made by summing the contributions of all the weak learners, typically weighted by their accuracy or learning rate.\n",
        "This process minimizes the loss function by using gradient descent, where each weak learner tries to reduce the residual errors from the previous learners, ultimately creating a highly accurate ensemble model."
      ],
      "metadata": {
        "id": "BUi5ldoy-Ch9"
      }
    }
  ]
}
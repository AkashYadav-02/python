{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Random Forest Regressor?\n",
        "\n",
        "Ans:\n",
        "A Random Forest Regressor is an ensemble machine learning algorithm that is used for regression tasks. It builds multiple decision trees during training and merges their results to improve the predictive accuracy and control overfitting. It works by averaging the outputs of several decision trees, which reduces variance and increases the robustness of the model."
      ],
      "metadata": {
        "id": "6nD9zAZQyPAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. How does Random Forest Regressor reduce the risk of overfitting?\n",
        "Random Forest Regressor reduces overfitting by:\n",
        "Ans:\n",
        "Ensemble learning: By combining multiple decision trees, it averages out the errors, making the model less sensitive to noise in the data.\n",
        "Bootstrap sampling: It uses random sampling (with replacement) to train each tree on a slightly different subset of the data, making each tree less likely to overfit the entire dataset.\n",
        "Random feature selection: At each split in a tree, it randomly selects a subset of features, reducing the chance of the trees overfitting to any particular feature."
      ],
      "metadata": {
        "id": "WaRqvnicyRXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?\n",
        "\n",
        "Ans:\n",
        "Random Forest Regressor aggregates predictions using a technique called bagging. After generating predictions from each individual tree, it calculates the mean of these predictions to make the final prediction. This averaging process helps to reduce the overall variance of the model, providing a more robust prediction."
      ],
      "metadata": {
        "id": "c2F_ffOAyVQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What are the hyperparameters of Random Forest Regressor?\n",
        "Some common hyperparameters of the Random Forest Regressor include:\n",
        "\n",
        "Ans:\n",
        "n_estimators: The number of trees in the forest.\n",
        "max_depth: The maximum depth of the trees.\n",
        "min_samples_split: The minimum number of samples required to split an internal node.\n",
        "min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
        "max_features: The number of features to consider when looking for the best split.\n",
        "bootstrap: Whether bootstrap samples are used when building trees.\n",
        "random_state: Seed for random number generation."
      ],
      "metadata": {
        "id": "-5Ir6r-IycF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?\n",
        "\n",
        "Ans:\n",
        "The main differences between Random Forest Regressor and Decision Tree Regressor are:\n",
        "\n",
        "Overfitting: A single decision tree can easily overfit the data, whereas Random Forest reduces overfitting by combining multiple decision trees.\n",
        "Accuracy: Random Forest is generally more accurate because it aggregates predictions from several trees, while a decision treeâ€™s accuracy can vary depending on the depth and structure of the tree.\n",
        "Model Complexity: Decision Trees are simpler and faster to train, but they are more prone to overfitting. Random Forest, being more complex with multiple trees, is slower to train but produces more accurate and stable results."
      ],
      "metadata": {
        "id": "wx3vLvWJyfzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. What are the advantages and disadvantages of Random Forest Regressor?\n",
        "\n",
        "Ans:\n",
        "\n",
        "Advantages:\n",
        "Reduces overfitting: By averaging multiple trees, it reduces the likelihood of overfitting to the training data.\n",
        "Handles large datasets well: It can handle large datasets and is efficient with high-dimensional data.\n",
        "Robust to noise: The model is generally resistant to noise and outliers in the data.\n",
        "No feature scaling required: Random Forests do not require normalization or scaling of the features.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Computationally expensive: It can be slower and more memory-intensive compared to individual decision trees.\n",
        "Model interpretability: The model is less interpretable compared to a single decision tree.\n",
        "Overfitting in certain cases: If the trees are not pruned properly, or if too many trees are used, overfitting might still occur."
      ],
      "metadata": {
        "id": "J6oW_6H-ykUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. What is the output of Random Forest Regressor?\n",
        "\n",
        "Ans.\n",
        "The output of a Random Forest Regressor is a continuous value that is the average of the predictions made by the individual decision trees in the forest. This output is used for regression tasks, where the goal is to predict a real-valued quantity."
      ],
      "metadata": {
        "id": "WduOskrCyqet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Can Random Forest Regressor be used for classification tasks?\n",
        "\n",
        "Ans:\n",
        "Yes, the Random Forest algorithm can be used for both regression and classification tasks. In the case of classification, the model aggregates the predictions from each decision tree using a majority vote (for classification tasks), instead of averaging the predictions as it does for regression."
      ],
      "metadata": {
        "id": "pRpPkf2oywGB"
      }
    }
  ]
}
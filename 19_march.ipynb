{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJlqJqAkXI6F"
      },
      "outputs": [],
      "source": [
        "Q1. The Filter method in feature selection involves evaluating the relevance of each feature independently of the predictive model. It ranks features based on certain criteria, such as correlation with the target variable or statistical significance, and selects the top-ranking features for inclusion in the model.\n",
        "\n",
        "Q2. The Wrapper method differs from the Filter method in that it evaluates subsets of features based on their performance with a specific machine learning algorithm. It selects features iteratively, using the predictive model's performance as a guide, and can consider interactions between features.\n",
        "\n",
        "Q3. Some common techniques used in Embedded feature selection methods include Lasso Regression, Ridge Regression, Elastic Net, Decision Trees, Random Forests, and Gradient Boosting Machines. These techniques embed feature selection within the model training process, allowing the algorithm to automatically select the most relevant features while building the model.\n",
        "\n",
        "Q4. Drawbacks of using the Filter method for feature selection include:\n",
        "\n",
        "It does not consider interactions between features.\n",
        "It may select redundant features that are highly correlated with each other.\n",
        "It may overlook features that are individually weak but collectively important.\n",
        "It does not directly optimize the performance of the predictive model.\n",
        "Q5. You might prefer using the Filter method over the Wrapper method for feature selection in situations where:\n",
        "\n",
        "You have a large dataset with many features, and you need a quick and computationally efficient way to reduce dimensionality.\n",
        "You want a simple and interpretable feature selection process that does not require extensive computational resources.\n",
        "You are primarily interested in identifying features that have a strong individual relationship with the target variable.\n",
        "Q6. To choose the most pertinent attributes for the customer churn predictive model using the Filter Method in the telecom company project:\n",
        "\n",
        "Calculate correlation coefficients or other relevant statistical measures between each feature and the target variable (churn).\n",
        "Rank the features based on their correlation or statistical significance.\n",
        "Select the top-ranking features for inclusion in the predictive model.\n",
        "Q7. To select the most relevant features for predicting the outcome of a soccer match using the Embedded method:\n",
        "\n",
        "Train a machine learning model (e.g., Random Forest or Gradient Boosting Machine) using all available features.\n",
        "Analyze the feature importances provided by the model.\n",
        "Select features with high importance scores as the most relevant for predicting the match outcome.\n",
        "Q8. To select the best set of features for predicting the price of a house using the Wrapper method:\n",
        "\n",
        "Start with an initial set of features.\n",
        "Train the predictive model using different combinations of features (subsets).\n",
        "Evaluate the performance of the model for each subset using a suitable evaluation metric (e.g., mean squared error for regression).\n",
        "Select the subset of features that results in the best model performance. You can use techniques like forward selection, backward elimination, or recursive feature elimination for this purpose."
      ]
    }
  ]
}